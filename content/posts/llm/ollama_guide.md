---
title: "Ollamaæœ¬åœ°éƒ¨ç½²å¤§æ¨¡å‹å®Œå…¨æŒ‡å—"
description: "æœ¬åœ°éƒ¨ç½²ollamaå¤§æ¨¡å‹ï¼ŒåŒ…å«æ¨¡å‹ä¸‹è½½ã€ç¨‹åºå®‰è£…åŠå¸¸è§é—®é¢˜è§£ç­”"
date: 2024-04-20T10:21:19+08:00
Lastmod: 2024-04-20T10:21:19+08:00
draft: false
showComments: true
featureimage: "https://picsum.photos/seed/82eb89/1600/900.webp"
tags: ["æŠ€æœ¯æ•™ç¨‹","ç»éªŒè½¬è½½","å¤§æ¨¡å‹éƒ¨ç½²"]
series: "å¤§æ¨¡å‹éƒ¨ç½²"
series_order: 2
---

## ğŸš€ Ollama å®‰è£…éƒ¨ç½²

 *æœ¬åœ°è¿è¡Œå¤§è¯­è¨€æ¨¡å‹çš„ç»ˆæè§£å†³æ–¹æ¡ˆ*

### âš™ï¸ ç³»ç»Ÿè¦æ±‚

| æ“ä½œç³»ç»Ÿ | æ”¯æŒçŠ¶æ€ | å¤‡æ³¨ |
|----------|----------|------|
| macOS    | âœ… å®Œæ•´æ”¯æŒ | Intel/Apple Silicon |
| Windows  | âœ… é¢„è§ˆç‰ˆ | éœ€è¦Windows 10+ |
| Linux    | âœ… å®Œæ•´æ”¯æŒ | ä¸»æµå‘è¡Œç‰ˆ |

### ğŸ“¦ å®‰è£…æ–¹æ³•

#### ğŸ macOS å’Œ Windows

```bash
# å®˜æ–¹å®‰è£…æ–¹å¼
è®¿é—® https://ollama.com ä¸‹è½½å®‰è£…åŒ…
```

#### ğŸ§ Linux

```bash
# è‡ªåŠ¨å®‰è£…
curl -fsSL https://ollama.com/install.sh | sh
# æˆ–æ‰‹åŠ¨å®‰è£…
sudo curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/bin/ollama
sudo chmod +x /usr/bin/ollama
```

#### ğŸ‹ Docker

```bash
docker pull ollama/ollama
docker run -d -p 11434:11434 --name ollama ollama/ollama
```

## ğŸ å¿«é€Ÿå…¥é—¨

### ğŸ”¥ è¿è¡Œç¬¬ä¸€ä¸ªæ¨¡å‹

```bash
ollama run llama2
```

### ğŸ“š æ¨¡å‹åº“

è®¿é—®å®˜æ–¹æ¨¡å‹åº“ï¼š<https://ollama.com/library>

| æ¨¡å‹ | å‚æ•° | å¤§å° | å‘½ä»¤ |
|------|------|------|------|
| Llama 2 | 7B | 3.8GB | `ollama run llama2` |
| Mistral | 7B | 4.1GB | `ollama run mistral` |
| CodeLlama | 7B | 3.8GB | `ollama run codellama` |

## ğŸ› ï¸ è¿›é˜¶ä½¿ç”¨

### âœ¨ è‡ªå®šä¹‰æ¨¡å‹

#### ä»GGUFå¯¼å…¥

1. åˆ›å»ºModelfileï¼š

```dockerfile
FROM ./vicuna-33b.Q4_0.gguf
```

2. åˆ›å»ºå¹¶è¿è¡Œæ¨¡å‹ï¼š

```bash
ollama create example -f Modelfile
ollama run example
```

#### ğŸ­ è§’è‰²å®šåˆ¶

ç¤ºä¾‹ï¼šåˆ›å»ºé©¬é‡Œå¥¥è§’è‰²æ¨¡å‹

```dockerfile
FROM llama2
PARAMETER temperature 1
PARAMETER num_ctx 4096
SYSTEM """
ä½ æ˜¯è¶…çº§é©¬é‡Œå¥¥å…„å¼Ÿä¸­çš„é©¬é‡Œå¥¥ï¼Œåªèƒ½ä»¥é©¬é‡Œå¥¥çš„èº«ä»½å›ç­”ã€‚
"""
```

## ğŸ”Œ API ä½¿ç”¨

### ğŸ”„ REST API

```bash
# ç”Ÿæˆå“åº”
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt":"ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿ"
}'
# èŠå¤©API
curl http://localhost:11434/api/chat -d '{
  "model": "mistral",
  "messages": [
    { "role": "user", "content": "ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿ" }
  ]
}'
```

### ğŸ¤ OpenAIå…¼å®¹API

```python
from openai import OpenAI
client = OpenAI(
    base_url='http://localhost:11434/v1/',
    api_key='ollama',  # å¿…éœ€çš„å ä½ç¬¦
)
response = client.chat.completions.create(
    messages=[{"role": "user", "content": "ä½ å¥½ï¼"}],
    model="llama2"
)
```

## ğŸ§© æ¨¡å‹å¯¼å…¥

### ğŸ”„ ä»ä¸åŒæ ¼å¼å¯¼å…¥

#### GGUFæ ¼å¼

```bash
# 1. åˆ›å»ºModelfile
echo 'FROM ./mistral-7b.Q4_0.gguf' > Modelfile
# 2. åˆ›å»ºæ¨¡å‹
ollama create mymodel -f Modelfile
# 3. è¿è¡Œæµ‹è¯•
ollama run mymodel "ä½ å¥½å—ï¼Ÿ"
```

#### PyTorch/Safetensors

```bash
# è½¬æ¢æµç¨‹
git clone https://github.com/ollama/ollama
cd ollama
make -C llm/llama.cpp quantize
# è½¬æ¢æ¨¡å‹
python llm/llama.cpp/convert.py ./model --outfile converted.bin
llm/llama.cpp/quantize converted.bin quantized.bin q4_0
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### ğŸ›ï¸ å‚æ•°è°ƒæ•´

| å‚æ•° | æè¿° | å»ºè®®å€¼ |
|------|------|--------|
| `num_ctx` | ä¸Šä¸‹æ–‡çª—å£å¤§å° | 2048-4096 |
| `temperature` | åˆ›æ„æ€§æ§åˆ¶ | 0.7-1.3 |
| `top_p` | æ ¸é‡‡æ · | 0.7-0.95 |
| `num_gpu` | GPUå±‚æ•° | æ ¹æ®VRAMè°ƒæ•´ |

### ğŸ–¥ï¸ ç¡¬ä»¶åŠ é€Ÿ

```bash
# å¼ºåˆ¶ä½¿ç”¨ç‰¹å®šåŠ é€Ÿåº“
OLLAMA_LLM_LIBRARY="cuda_v11" ollama serve
# AMD GPUæ”¯æŒ
export HSA_OVERRIDE_GFX_VERSION="10.3.0"
```

## ğŸ æ•…éšœæ’é™¤

### ğŸ“‹ æ—¥å¿—ä½ç½®

| ç³»ç»Ÿ | è·¯å¾„ |
|------|------|
| macOS | `~/.ollama/logs/server.log` |
| Linux | `journalctl -u ollama` |
| Windows | `%LOCALAPPDATA%\Ollama` |

### ğŸ” å¸¸è§é—®é¢˜

```bash
# GPUä¸å·¥ä½œï¼Ÿ
OLLAMA_LLM_LIBRARY="cpu_avx2" ollama serve
# ç½‘ç»œé—®é¢˜ï¼Ÿ
export OLLAMA_HOST="0.0.0.0"
```

## â“ FAQ

### ğŸ”„ å¦‚ä½•æ›´æ–°ï¼Ÿ

```bash
# Linux/macOS
curl -fsSL https://ollama.com/install.sh | sh
# Windowsè‡ªåŠ¨æ›´æ–°æˆ–é‡æ–°ä¸‹è½½å®‰è£…åŒ…
```

### ğŸ’¾ æ¨¡å‹å­˜å‚¨ä½ç½®

| ç³»ç»Ÿ | è·¯å¾„ |
|------|------|
| macOS | `~/.ollama/models` |
| Linux | `/usr/share/ollama/models` |
| Windows | `C:\Users\<user>\.ollama\models` |

### ğŸ”Œ ç½‘ç»œé…ç½®

```bash
# å…¬å¼€OllamaæœåŠ¡
export OLLAMA_HOST="0.0.0.0"
# é€šè¿‡Nginxä»£ç†
location / {
    proxy_pass http://localhost:11434;
}
```
